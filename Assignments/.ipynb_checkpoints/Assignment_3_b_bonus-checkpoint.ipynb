{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 part 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task HPV slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "There's a whole thesis written based on this dataset, available here: http://uu.diva-portal.org/smash/get/diva2:1650957/FULLTEXT01.pdf . The thesis then resulted in a paper: https://openaccess.thecvf.com/content_ICCV_2017_workshops/w1/html/Wieslander_Deep_Convolutional_Neural_ICCV_2017_paper.html\n",
    "\n",
    "\n",
    "A quote from Weislander et al. regarding the dataset:\n",
    "\n",
    "\"The cell samples were collected at Södersjukhuset in Stockholm. The patients have mixed genders, are non smoking, some are human papillomavirus (HPV) positive and some are not, and they have an age span of 47-77 years. From each patient samples were collected with a brush that is scraped at areas of interest in the oral cavity. Each scrape is then smeared out on a glass, which is then stained to highlight important cellular structures\"\n",
    "\n",
    "## Notes\n",
    "This assignment is inspired by Phil Harrisons lab for Pharmaceutical Bioinformatics and Sequence Analysis  from 2021\n",
    "\n",
    "**You are not allowed to use the exact same network and configurations as in these texts, or as in your hand in for the mandatory assignment, but you are allowed to use the same base network if you are using transfer learning.**  \n",
    "\n",
    "You may discuss theory with other groups, but not code nor share code. You may not use ChatGPT4 or similar programs to generate a solution or code.\n",
    "\n",
    " <span style=\"color:red\"> Solutions deemed to similar will be run through a program designed to detect coding plagirism and if flagged you will be reported for plagirism. </span> \n",
    " \n",
    " One hand in by the group.\n",
    " \n",
    " ## Hand in 2024\n",
    " Hand in one pdf file and one .ipynb file. The notebook must be runnable with the data, any teacher might run the notebook to check that it actually works. Notebooks that do not run will recieve no points\n",
    " \n",
    "## 1 bonuspoint:\n",
    "A neural network that achieve a 80% accuracy on the HPV data \n",
    "\n",
    "## 1 bonuspoint:\n",
    "Answer the questions below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Questions\n",
    "#### HPV models\n",
    "Q1: If a clinician requires a 95% accuracy in their models, and you've achieved this accuracy, would you recommend the model you have generated? Why or why not? (if you have not generated the model discuss the worst case senario for 2 other measurements to use in your discussion)\n",
    "\n",
    "Q3: In this previous excercise we have mixed the patients all together. What is the main drawback with this type of datamixing?\n",
    "\n",
    "\n",
    "Q4: What is the benefit of us mixing the patients?\n",
    "\n",
    "#### Zebra fish paper\n",
    "\n",
    "\n",
    "Q5: Zebra-fish paper  \n",
    "    What is the size of the data used in Deep Fish: Deep Learning–Based Classification of Zebrafish Deformation for High-Throughput Screening by Ishaq, Sandanandan et.al ? https://journals.sagepub.com/doi/full/10.1177/1087057116667894 \n",
    "\n",
    "Q6: Zebra-fish paper   \n",
    "       What architecture did they use? Did they use transferlearning?\n",
    "\n",
    "Q7: Zebra-fish paper  \n",
    "     What did they do in order to ease concerns about what the neural network actually learns? What interesting thing did they find?\n",
    "     \n",
    "Q8: If you were to improve these networks further, what 3 strategies would you start with? Take into account the strategies the paper used, and think of different ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import IPython\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GPUs\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      print(\"Done setting memory_growth\")\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except Exception as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(\"If you get this , probably done already\")\n",
    "    # Catch the exception and display a custom HTML message with red text\n",
    "    message = \"\"\"There was some problems setting up the GPU,\n",
    "                 it is probably best to restart kernel and clear\n",
    "                 all outputs before starting over\n",
    "              \"\"\"\n",
    "    display(HTML(f\"<div style='color: red;'><strong>Warning:</strong>{message}</div>\"))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = \"/home/jovyan/Teaching/BigDataDL/\" # add the full path printed above here, but put it one step above \"Assignments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from importlib.machinery import SourceFileLoader\n",
    "lab_path = base_directory + \"Labs/\"\n",
    "\n",
    "cnn_helper = SourceFileLoader(\"cnn_helper\", lab_path + \"cnn_helper.py\").load_module()\n",
    "plot_helper = SourceFileLoader(\"plot_helper\", lab_path + \"plot_helper.py\").load_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "Don't worry about the functions, they'll make more sense later. Just keep moving along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_time():\n",
    "    print(\"Starting run at: \" + str(datetime.now()))\n",
    "\n",
    "def end_time():\n",
    "    print(\"Run finished at: \" + str(datetime.now()))\n",
    "    \n",
    "def get_image_data_flat_from_file(data_directory, image_paths):\n",
    "    file_names = image_paths.values.flatten() # Assumes image_paths come in df[image_path_column_name] structure due to lab\n",
    "    image_data = np.array([np.array(cv2.imread(data_directory + file_name)) for file_name in file_names])\n",
    "    flattened_image_data = image_data.reshape(image_data.shape[0], -1)\n",
    "    return flattened_image_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def valid_evaluate(model):\n",
    "    y_pred = model.predict_generator(validation_generator, validation_steps+1)\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_true = validation_generator.classes\n",
    "    class_names = ['healthy', 'tumor']\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred, normalize=\"all\")\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.figure(figsize=(15,5), facecolor='w')\n",
    "    plot_helper.plot_confusion_matrix(cnf_matrix, classes=class_names, title='confusion matrix for validation data')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "\n",
    "def test_evaluate(model):\n",
    "    y_pred = model.predict_generator(test_generator, test_steps+1)\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_true = test_generator.classes\n",
    "    class_names = ['healthy', 'tumor']\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred, normalize=\"all\")\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.figure(figsize=(15,5), facecolor='w')\n",
    "    plot_helper.plot_confusion_matrix(cnf_matrix, classes=class_names, title='confusion matrix for test data')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extraced the HPV data in Zipped data to LabData, then run the HPV_v3_data_prep.ipynb notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# director whit our dataset\n",
    "hpv_images_directory = base_directory+ 'LabData/HPV_slides/'\n",
    "\n",
    "# directories for our training, validation and test splits\n",
    "train_directory = os.path.join(hpv_images_directory, 'train')\n",
    "validation_directory = os.path.join(hpv_images_directory, 'validation')\n",
    "test_directory = os.path.join(hpv_images_directory, 'test')\n",
    "\n",
    "# directory with our training healthy cell images\n",
    "train_healthy_directory = os.path.join(train_directory, 'healthy')\n",
    "\n",
    "# directory with our training tumor cell images\n",
    "train_tumor_directory = os.path.join(train_directory, 'tumor')\n",
    "\n",
    "# directory with our validation healthy cell images\n",
    "validation_healthy_directory = os.path.join(validation_directory, 'healthy')\n",
    "\n",
    "# directory with our validation tumor cell images\n",
    "validation_tumor_directory = os.path.join(validation_directory, 'tumor')\n",
    "\n",
    "# directory with our test healthy cell images\n",
    "test_healthy_directory = os.path.join(test_directory, 'healthy')\n",
    "\n",
    "# directory with our test tumor cell images\n",
    "test_tumor_directory = os.path.join(test_directory, 'tumor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Data check\n",
    "\n",
    "print('Number of  healthy images for training:', len(os.listdir(train_healthy_directory)))\n",
    "print('Number of  tumor images for training:', len(os.listdir(train_tumor_directory)))\n",
    "print('')\n",
    "print('Number of  healthy images for validation:', len(os.listdir(validation_healthy_directory)))\n",
    "print('Number of  tumor imagess for validation:', len(os.listdir(validation_tumor_directory)))\n",
    "print('')\n",
    "print('Number of  healthy images for testing:', len(os.listdir(test_healthy_directory)))\n",
    "print('Number of  tumor images for testing:', len(os.listdir(test_tumor_directory)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot sample images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_helper.show_random_images_hpv_specific(train_healthy_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = 48\n",
    "y_len = 48\n",
    "batch_size = 32\n",
    "n_epochs = 60\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print('TRAINING DATA:')\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size=(x_len, y_len),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary')\n",
    "\n",
    "print('')\n",
    "print('VALIDATION DATA:')\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_directory,\n",
    "    target_size=(x_len, y_len),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary',\n",
    "    shuffle=False)\n",
    "\n",
    "print('')\n",
    "print('TEST DATA:')\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_directory,\n",
    "    target_size=(x_len, y_len),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='binary',\n",
    "    shuffle=False)\n",
    "\n",
    "class_weights_array = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes)\n",
    "class_weights = dict(enumerate(class_weights_array))\n",
    "\n",
    "\n",
    "train_steps = (len(os.listdir(train_healthy_directory)) + len(os.listdir(train_tumor_directory))) // batch_size\n",
    "validation_steps = (len(os.listdir(validation_healthy_directory)) + len(os.listdir(validation_tumor_directory))) // batch_size\n",
    "test_steps = (len(os.listdir(test_healthy_directory)) + len(os.listdir(test_tumor_directory))) // batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STUDENT CODE BEGINS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first-try neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Final neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Explain why your chose the architecture you did, and the extra steps you took\n",
    "If you made more than 2 neural networks, you only need to show the first and last one. Do motivate any changes in between the two, in short sentences so that I can see that you know why you did what you did"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
